
import { getHotState, saveHotState, addMessageToState as addMessage } from '@/lib/state/redis-state';
import { getUserMemory, learnFromInteraction } from '@/lib/memory/memory-manager';
import { verifyResponse } from '@/lib/verification/verifier';
import { decideEscalation } from '@/lib/verification/escalation';
import { detectAgentFromImage, validateAttachment } from '@/lib/attachment/attachment-manager';
import { canAnalyzeImages, validateImageForAgent, AGENT_BOUNDARIES } from '@/lib/agent/boundaries';
import { buildKBContext, getKBStats } from '@/lib/agent/knowledge-manager';
import Anthropic from '@anthropic-ai/sdk';
import { AgentMemory as UserMemory, VerificationResult, Attachment } from '@/lib/types';
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
import path from 'path';

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY! });

// Resolve MCP server path relative to backend root (not process.cwd())
const BACKEND_ROOT = path.resolve(__dirname, '../..');
const MCP_SERVERS_DIR = path.join(BACKEND_ROOT, 'mcp-servers');

export interface AgentRequest {
  conversationId: string;
  userId: string;
  agentType: string;
  userMessage: string;
  attachments?: Attachment[];
  autoDetectAgent?: boolean; // Auto-detect agent from attachments
}

export interface AgentResponse {
  message: string;
  confidence: number;
  verified: boolean;
  escalated: boolean;
  escalationReason?: string;
  warnings: string[];
  detectedAgent?: string; // If auto-detected from image
  metadata: {
    tokensUsed: number;
    processingTime: number;
    retryCount: number;
    attachmentsProcessed?: number;
    visionAnalysesCreated?: number; // Phase 2: Number of vision analyses
    kbEntriesCreated?: number; // Phase 2: Number of KB entries from images
  };
}

/**
 * Auto-detect agent type from attachments
 */
function autoDetectAgent(attachments?: Attachment[], currentAgent?: string): string {
  if (!attachments || attachments.length === 0) {
    return currentAgent || 'coder';
  }

  // Check first attachment
  const detected = detectAgentFromImage(attachments[0].filename, attachments[0].mimeType);
  return detected || currentAgent || 'coder';
}

/**
 * Build vision-enabled message content with attachments
 * Phase 2: Supports multiple images (up to 5), real base64 encoding, and vision analysis
 */
function buildVisionContent(
  userMessage: string,
  attachments?: Attachment[]
): Anthropic.MessageParam['content'] {
  if (!attachments || attachments.length === 0) {
    return userMessage;
  }

  const content: Array<Anthropic.TextBlockParam | Anthropic.ImageBlockParam> = [
    {
      type: 'text',
      text: userMessage,
    },
  ];

  // Phase 2: Support up to 5 images per message
  const maxImages = 5;
  for (let i = 0; i < Math.min(attachments.length, maxImages); i++) {
    const att = attachments[i];

    // Check if we have visionAnalysis data to build richer context
    if (att.visionAnalysis) {
      // Add vision analysis as text context before image
      const visionData = typeof att.visionAnalysis === 'string'
        ? att.visionAnalysis
        : att.visionAnalysis.summary || att.visionAnalysis.analysis?.substring(0, 100);
      const analysisContext = `[Image ${i + 1} Analysis: ${visionData}]`;
      content.push({
        type: 'text',
        text: analysisContext,
      });
    }

    // Add image (now with real base64 or storage URL)
    // Note: The attachment should already have base64 data or valid URL
    content.push({
      type: 'image',
      source: {
        type: 'base64',
        media_type: att.mimeType as 'image/jpeg' | 'image/png' | 'image/gif' | 'image/webp',
        // In Phase 2, this should be actual base64 data, not placeholder
        // Extract from metadata or use empty string as fallback
        data: (att.metadata?.base64 as string) || 'placeholder-base64-data',
      } as any, // Type assertion needed due to Anthropic SDK limitations
    });
  }

  return content;
}

/**
 * Process attachments with vision analysis (Phase 2)
 * Analyzes all attachments and optionally creates KB entries for high-confidence results
 */
async function processAttachmentsWithVision(
  attachments: Attachment[],
  agentType: string,
  conversationId: string
): Promise<{
  processedImages: Attachment[];
  analyses: Array<any>;
  kbEntriesCreated: number;
}> {
  const { analyzeImageWithVision } = await import('@/lib/attachment/vision-analyzer');
  const { createKBEntryFromImage } = await import('@/lib/attachment/summarizer');
  const { saveVisionAnalysis } = await import('@/lib/attachment/attachment-persistence');

  const analyses = [];
  let kbEntriesCreated = 0;

  for (const att of attachments) {
    try {
      // Skip if already analyzed
      if (att.visionAnalysis) {
        analyses.push(att.visionAnalysis);
        continue;
      }

      // Analyze with vision API
      const analysis = await analyzeImageWithVision(
        att.metadata?.base64 || att.url,
        att.mimeType,
        agentType as any,
        'Analyze this image in detail'
      );

      analyses.push(analysis);

      // Save analysis to database
      try {
        await saveVisionAnalysis(att.id, agentType as any, analysis);
      } catch (error) {
        console.warn('Failed to save vision analysis:', error);
      }

      // Try to create KB entry if confidence is high
      if (analysis.confidence > 75) {
        try {
          const kbEntry = await createKBEntryFromImage(att.id, agentType as any, analysis);
          if (kbEntry) {
            kbEntriesCreated++;
          }
        } catch (error) {
          console.warn('Failed to create KB entry from image:', error);
        }
      }
    } catch (error) {
      console.error('Failed to process attachment:', error);
      analyses.push({
        analysis: 'Failed to analyze',
        summary: 'Error during vision analysis',
        detectedType: 'other',
        confidence: 0,
        keyPoints: [],
        metadata: { error: String(error) },
      });
    }
  }

  return {
    processedImages: attachments,
    analyses,
    kbEntriesCreated,
  };
}

export async function processAgentRequest(
  request: AgentRequest
): Promise<AgentResponse> {

  const startTime = Date.now();
  let retryCount = 0;
  let detectedAgent: string | undefined;

  // 0. Auto-detect agent if requested
  let finalAgentType = request.agentType;
  if (request.autoDetectAgent && request.attachments) {
    detectedAgent = autoDetectAgent(request.attachments, request.agentType);
    finalAgentType = detectedAgent;
  }

  // 1. Load state + memory
  let state = await getHotState(request.conversationId);
  if (!state) {
    state = {
      conversationId: request.conversationId,
      userId: request.userId,
      agentType: finalAgentType,
      messages: [],
      context: {},
      metadata: {
        startedAt: new Date().toISOString(),
        lastMessageAt: new Date().toISOString(),
        messageCount: 0,
        tokensUsed: 0
      }
    };
  }

  const memory = await getUserMemory(request.userId, finalAgentType);

  // 2. Add user message to state
  await addMessage(request.conversationId, {
    role: 'user',
    content: request.userMessage,
    attachments: request.attachments
  });

  // 3. Build memory-enhanced system prompt with KB context
  const systemPrompt = await buildSmartPrompt(finalAgentType, memory);

  // 4. Setup MCP Connectivity
  let mcpClient: Client | null = null;
  let mcpTransport: StdioClientTransport | null = null;
  let anthropicTools: Anthropic.Tool[] = [];

  try {
    const serverPath = path.join(MCP_SERVERS_DIR, `${finalAgentType}-agent`, 'server.ts');
    mcpTransport = new StdioClientTransport({ command: "npx", args: ["tsx", serverPath] });
    mcpClient = new Client({ name: "orchestrator", version: "1.0.0" }, { capabilities: {} });
    await mcpClient.connect(mcpTransport);
    const toolsList = await mcpClient.listTools();
    anthropicTools = toolsList.tools.map(t => ({
      name: t.name,
      description: t.description || "",
      input_schema: t.inputSchema as Anthropic.Tool["input_schema"]
    }));
  } catch (e) {
    console.warn("MCP Connection Failed (running without tools):", e);
  }

  // 5. Call Claude with context
  const contextWindow = state.messages.slice(-10);

  let assistantResponse: string = "";
  let confidence: number = 0;
  let verificationResult: VerificationResult = { passed: false, confidence: 0, issues: [], warnings: [], shouldEscalate: false };
  let escalationDecision: { shouldEscalate: boolean; reason?: string } = { shouldEscalate: false };

  try {
    // üî• Hydrate attachments from database (fetch base64 data)
    // This prevents storing large base64 strings in Redis
    const { getAttachmentsByIds } = await import('@/lib/attachment/attachment-persistence');

    // Collect all attachment IDs from context window
    const allAttachmentIds: string[] = [];
    for (const msg of contextWindow) {
      if (msg.attachments && msg.attachments.length > 0) {
        for (const att of msg.attachments) {
          if (att.id) {
            allAttachmentIds.push(att.id);
          }
        }
      }
    }

    // Fetch all attachments with base64 data in one query
    const hydratedAttachments = await getAttachmentsByIds(allAttachmentIds);
    const attachmentMap = new Map(hydratedAttachments.map(att => [att.id, att]));

    // Retry loop (max 2 attempts)
    while (retryCount < 2) {
      const messages: Anthropic.MessageParam[] = [
        ...contextWindow.map(m => {
          // Hydrate attachments with base64 from database
          let hydratedMessageAttachments = m.attachments;
          if (m.attachments && m.attachments.length > 0) {
            hydratedMessageAttachments = m.attachments.map(att => {
              const fullAttachment = attachmentMap.get(att.id);
              return fullAttachment || att; // Use full data if available
            });
          }

          return {
            role: m.role as 'user' | 'assistant',
            // Include attachments in context for user messages
            content: m.role === 'user' && hydratedMessageAttachments && hydratedMessageAttachments.length > 0
              ? buildVisionContent(m.content, hydratedMessageAttachments)
              : m.content
          };
        }),
        {
          role: 'user' as const,
          content: buildVisionContent(request.userMessage, request.attachments)
        }
      ];

      const claudeResponse = await anthropic.messages.create({
        model: "claude-haiku-4-5-20251001",
        max_tokens: 2048,
        system: systemPrompt,
        tools: anthropicTools.length > 0 ? anthropicTools : undefined,
        messages
      });

      // Handle Tool Use - feed result back to Claude
      if (claudeResponse.stop_reason === "tool_use" && mcpClient) {
        const toolUse = claudeResponse.content.find(c => c.type === "tool_use");
        if (toolUse && toolUse.type === "tool_use") {
          let toolResultContent: string;
          try {
            const toolResult = await mcpClient.callTool({ name: toolUse.name, arguments: toolUse.input as Record<string, unknown> });
            toolResultContent = JSON.stringify(toolResult.content);
            state.context = { ...state.context, [toolUse.name]: toolResult };
          } catch (e) {
            toolResultContent = `Error executing tool: ${e}`;
          }

          // Feed tool result back to Claude for a proper response
          const followUp = await anthropic.messages.create({
            model: "claude-haiku-4-5-20251001",
            max_tokens: 2048,
            system: systemPrompt,
            tools: anthropicTools,
            messages: [
              ...messages,
              { role: 'assistant', content: claudeResponse.content },
              {
                role: 'user', content: [{
                  type: 'tool_result',
                  tool_use_id: toolUse.id,
                  content: toolResultContent
                }]
              }
            ]
          });

          const textBlock = followUp.content.find(c => c.type === "text");
          assistantResponse = textBlock && textBlock.type === "text" ? textBlock.text : toolResultContent;
          state.metadata.tokensUsed += followUp.usage.input_tokens + followUp.usage.output_tokens;
        }
      } else {
        const textBlock = claudeResponse.content.find(c => c.type === "text");
        assistantResponse = textBlock && textBlock.type === "text" ? textBlock.text : "Processed.";
      }

      const tokensUsed = claudeResponse.usage.input_tokens + claudeResponse.usage.output_tokens;
      state.metadata.tokensUsed += tokensUsed;

      // 6. Verify response
      verificationResult = await verifyResponse(
        finalAgentType,
        assistantResponse,
        state.context,
        memory
      );

      confidence = verificationResult.confidence;

      // 7. Escalation decision
      escalationDecision = await decideEscalation(confidence, verificationResult.issues, memory);

      if (verificationResult.passed || retryCount >= 1) {
        break;
      }

      retryCount++;
    }
  } finally {
    // Cleanup MCP process to prevent zombie processes
    if (mcpClient) {
      try { await mcpClient.close(); } catch { /* ignore cleanup errors */ }
    }
  }

  // 8. Save assistant message
  await addMessage(request.conversationId, {
    role: 'assistant',
    content: assistantResponse,
    metadata: {
      confidence,
      verified: verificationResult.passed,
      warnings: verificationResult.warnings
    }
  });

  // 9. Save final state
  await saveHotState(state);

  // 10. Async: Learn from this interaction
  learnFromInteraction(request.userId, finalAgentType, {
    userMessage: request.userMessage,
    assistantResponse,
    wasApproved: !escalationDecision.shouldEscalate,
    confidence
  }).catch(console.error);

  // 11. Return response
  return {
    message: assistantResponse,
    confidence,
    verified: verificationResult.passed,
    escalated: escalationDecision.shouldEscalate,
    escalationReason: escalationDecision.reason,
    warnings: verificationResult.warnings,
    detectedAgent: detectedAgent,
    metadata: {
      tokensUsed: state.metadata.tokensUsed,
      processingTime: Date.now() - startTime,
      retryCount,
      attachmentsProcessed: request.attachments?.length || 0
    }
  };
}

// Build smart prompt with memory and knowledge base
async function buildSmartPrompt(agentType: string, memory: UserMemory): Promise<string> {
  const basePrompts: Record<string, string> = {
    design: `‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Senior Creative Director ‡πÅ‡∏•‡∏∞ UI/UX Expert ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÇ‡∏•‡∏Å
- ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏£‡∏π‡∏´‡∏£‡∏≤ (Premium), ‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢ (Modern), ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á (Usability)
- ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠: ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏î‡πâ‡∏≤‡∏ô‡∏à‡∏¥‡∏ï‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÄ‡∏™‡∏°‡∏≠
- ‡∏†‡∏≤‡∏©‡∏≤: ‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡∏ä‡∏≠‡∏ö‡πÉ‡∏ä‡πâ‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢`,

    analyst: `‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Lead Data Scientist ‡πÅ‡∏•‡∏∞‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå
- ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data-Driven), ‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á (Risk Assessment), ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤ (ROI)
- ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠: ‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Bullet points) ‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
- ‡∏†‡∏≤‡∏©‡∏≤: ‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤ ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á`,

    coder: `‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Full-Stack Software Architect ‡πÅ‡∏•‡∏∞ AI Automation Expert
- ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Code ‡∏ó‡∏µ‡πà Clean, Scalable ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á Security ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á
- ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Logic ‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Workflow) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö
- ‡∏†‡∏≤‡∏©‡∏≤: ‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Solution-oriented)`,

    marketing: `‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Strategic Marketing Manager ‡πÅ‡∏•‡∏∞ Content Creator ‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
- ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢ (Conversion), ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á (Storytelling), ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå (Branding)
- ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠: ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏µ‡πÄ‡∏≠‡∏ó‡∏µ‡∏ü ‡∏°‡∏µ‡∏û‡∏•‡∏±‡∏á (Energetic) ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏™‡∏°‡∏≠
- ‡∏†‡∏≤‡∏©‡∏≤: ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏î‡∏π‡∏î‡πÉ‡∏à (Copywriting execution) ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á`
  };

  let prompt = basePrompts[agentType] || "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô";

  // ‡πÄ‡∏û‡∏¥‡πà‡∏° Global Logic (Smart Lazy Style)
  prompt += `\n\nüìå ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Smart Lazy Style):
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö
- ‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏•‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (80/20 Rule)
- ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏à‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
- ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ)`;

  // ‡πÄ‡∏û‡∏¥‡πà‡∏° Knowledge Base Context
  try {
    const kbContext = await buildKBContext(agentType as any);
    if (kbContext) {
      prompt += `\n\n${kbContext}`;
    }

    // ‡πÄ‡∏û‡∏¥‡πà‡∏° KB Statistics
    const stats = await getKBStats(agentType as any);
    if (stats.totalEntries > 0) {
      prompt += `\nüìä Knowledge Base Summary: ${stats.totalEntries} entries across ${stats.categoriesCount} categories`;
    }
  } catch (error) {
    console.warn('Failed to load KB context:', error);
  }

  if (memory.preferences) {
    prompt += `\n\nüë§ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (User Memory):`;
    if (memory.preferences.outputStyle) prompt += `\n- ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏≠‡∏ö: ${memory.preferences.outputStyle}`;
    if (memory.preferences.tone) prompt += `\n- ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: ${memory.preferences.tone}`;
  }

  const topPatterns = memory.patterns
    .filter(p => p.confidence > 0.75)
    .slice(0, 5);

  if (topPatterns.length > 0) {
    prompt += `\n\nüß† ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì:`;
    topPatterns.forEach(p => {
      prompt += `\n- ${p.pattern}`;
    });
  }

  prompt += `\n\n‚úÖ ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û:
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Self-verified)
- ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Knowledge Base ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á`;

  return prompt;
}
